{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cxz/miniconda3/envs/new/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package stopwords to /home/cxz/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import jsonlines\n",
    "import csv\n",
    "import re\n",
    "import time\n",
    "import argparse\n",
    "import sys\n",
    "import sklearn\n",
    "import traceback\n",
    "\n",
    "from torch.utils import data\n",
    "from tqdm import tqdm\n",
    "# from apex import amp\n",
    "from scipy.special import softmax\n",
    "\n",
    "from ditto_light.ditto import evaluate, DittoModel\n",
    "from ditto_light.exceptions import ModelNotFoundError\n",
    "from ditto_light.dataset import DittoDataset\n",
    "from ditto_light.summarize import Summarizer\n",
    "from ditto_light.knowledge import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 假设 ent1 和 ent2 是 DataFrame 数据\n",
    "# 创建示例 DataFrame\n",
    "ent1 = pd.DataFrame({'attr1': [1, 2, 3], 'attr2': ['A', 'B', 'C']})\n",
    "ent2 = pd.DataFrame({'attr1': [4, 5, 6], 'attr2': ['D', 'E', 'F']})\n",
    "\n",
    "content = ''\n",
    "for ent in [ent1, ent2]:\n",
    "    if isinstance(ent, pd.DataFrame):\n",
    "        for index, row in ent.iterrows():\n",
    "            for attr in ent.columns:\n",
    "                content += 'COL %s VAL %s ' % (attr, row[attr])\n",
    "            content += '\\t'\n",
    "    else:\n",
    "        if isinstance(ent, str):\n",
    "            content += ent\n",
    "        else:\n",
    "            for attr in ent.keys():\n",
    "                content += 'COL %s VAL %s ' % (attr, ent[attr])\n",
    "        content += '\\t'\n",
    "\n",
    "content += '0'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   attr1 attr2\n",
       " 0      1     A\n",
       " 1      2     B\n",
       " 2      3     C,\n",
       "    attr1 attr2\n",
       " 0      4     D\n",
       " 1      5     E\n",
       " 2      6     F)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ent1, ent2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'COL attr1 VAL 1 COL attr2 VAL A \\tCOL attr1 VAL 2 COL attr2 VAL B \\tCOL attr1 VAL 3 COL attr2 VAL C \\tCOL attr1 VAL 4 COL attr2 VAL D \\tCOL attr1 VAL 5 COL attr2 VAL E \\tCOL attr1 VAL 6 COL attr2 VAL F \\t0'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int):\n",
    "    \"\"\"\n",
    "    Helper function for reproducible behavior to set the seed in ``random``, ``numpy``, ``torch``\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 支持多种数据输入\n",
    "# txt文件\n",
    "# jsonl文件\n",
    "# string -pairs\n",
    "# csv 文件\n",
    "\n",
    "# 序列化 一对 数据条目\n",
    "def to_str(ent1, ent2):\n",
    "\n",
    "    # jsonl :{'title': '  \"GoPro Headstrap Plus Quickclip\"@en Quickclip | Sportsman\\'s Warehouse\"@en'}\n",
    "    # content :'COL title VAL   \"GoPro Headstrap Plus Quickclip\"@en Quickclip | Sportsman\\'s Warehouse\"@en '\n",
    "    content = ''\n",
    "    for ent in [ent1, ent2]:\n",
    "        if isinstance(ent, str):\n",
    "            content += ent\n",
    "        else:\n",
    "            for attr in ent.keys():\n",
    "                content += 'COL %s VAL %s ' % (attr, ent[attr])\n",
    "        content += '\\t'\n",
    "\n",
    "    content += '0'\n",
    "    # 每个属性之间用空格隔开，两个数据条目之间用制表符隔开。\n",
    "    # 向content中添加字符\"0\"，表示序列化后的字符串末尾\n",
    "\n",
    "    new_ent1, new_ent2, _ = content.split('\\t')\n",
    "\n",
    "    return new_ent1 + '\\t' + new_ent2 + '\\t0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove 1.txt\n"
     ]
    }
   ],
   "source": [
    "input_path = '1.txt'\n",
    "input_path += '.jsonl'\n",
    "\n",
    "print('remove', input_path[:-6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 的句子对应用于MRPC模型进行分类,#返回预测标签啊对应的得分\n",
    "def classify(sentence_pairs, model,\n",
    "             lm='distilbert',\n",
    "             max_len=256,\n",
    "             threshold=None): \n",
    "    \"\"\"Apply the MRPC model.\n",
    "\n",
    "    Args:\n",
    "        sentence_pairs (list of str): the sequence pairs\n",
    "        model (MultiTaskNet): the model in pytorch\n",
    "        max_len (int, optional): the max sequence length\n",
    "        threshold (float, optional): the threshold of the 0's class\n",
    "\n",
    "    Returns:\n",
    "        list of float: the scores of the pairs\n",
    "    \"\"\"\n",
    "    inputs = sentence_pairs\n",
    "    # print('max_len =', max_len)\n",
    "    dataset = DittoDataset(inputs,\n",
    "                           max_len=max_len,\n",
    "                           lm=lm)\n",
    "    # dataset = DittoDataset(inputs,\n",
    "    #                        max_len=max_len) lm = roberta\n",
    "    # print(dataset[0])\n",
    "    iterator = data.DataLoader(dataset=dataset,\n",
    "                               batch_size=len(dataset), # 一批次？\n",
    "                               shuffle=False,\n",
    "                               num_workers=0,\n",
    "                               collate_fn=DittoDataset.pad)\n",
    "    # prediction\n",
    "    all_probs = []\n",
    "    all_logits = []\n",
    "    with torch.no_grad():\n",
    "        # print('Classification')\n",
    "        for i, batch in enumerate(iterator):\n",
    "            x, _ = batch\n",
    "            logits = model(x)\n",
    "            probs = logits.softmax(dim=1)[:, 1]\n",
    "            all_probs += probs.cpu().numpy().tolist()\n",
    "            all_logits += logits.cpu().numpy().tolist()\n",
    "\n",
    "# 默认为0.5\n",
    "    if threshold is None:\n",
    "        threshold = 0.5\n",
    "\n",
    "    pred = [1 if p > threshold else 0 for p in all_probs]\n",
    "    return pred, all_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#　模型预测，并将预测结果写入　输出文件\n",
    "def predict(input_rows, output_path,\n",
    "            model,\n",
    "            batch_size=1024,\n",
    "            lm='distilbert',\n",
    "            threshold=None):\n",
    "\n",
    "    pairs = []\n",
    "# 处理数据批次的预测结果\n",
    "#　直接写入　使用writer(写入了output-json文件)\n",
    "    def process_batch(pairs, writer):\n",
    "        predictions, logits = classify(pairs, model, lm=lm,\n",
    "                                       threshold=threshold)\n",
    "        # logits是模型对输入句子对的预测结果，表示两个句子是同义词和不是同义词的概率得分\n",
    "        #   通常是一个包含两个值的数组\n",
    "        \n",
    "        # scores是通过对logits进行softmax操作得到的类别概率分布\n",
    "\n",
    "        scores = softmax(logits, axis=1)\n",
    "        for pair, pred, score in zip(pairs, predictions, scores):\n",
    "            output = {'left': pair[0], 'right': pair[1],\n",
    "                'match': pred,\n",
    "                'match_confidence': score[int(pred)]}\n",
    "            writer.write(output)\n",
    "    \n",
    "    # 将结果写入jsonl文件\n",
    "    rows = input_rows\n",
    "    with jsonlines.open(output_path, mode='w') as writer: # writer向文件写入\n",
    "        pairs = []\n",
    "        for row in rows:\n",
    "            pairs.append(to_str(row[0], row[1]))\n",
    "            \n",
    "            # 满足batch_size 进行批量处理\n",
    "            if len(pairs) == batch_size:\n",
    "                # predict batch_size 批量处理\n",
    "                process_batch(pairs, writer)\n",
    "                pairs.clear()\n",
    "\n",
    "        if len(pairs) > 0:\n",
    "            process_batch(pairs, writer)\n",
    "       \n",
    "    # run_time = time.time() - start_time\n",
    "    # run_tag = '%s_lm=%s_dk=%s_su=%s' % (config['name'], lm, str(dk_injector != None), str(summarizer != None))\n",
    "    # os.system('echo %s %f >> log.txt' % (run_tag, run_time))\n",
    "    # # 产生一个log.txt文件 将运行标签和运行时间写入到log.txt文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 加载模型\n",
    "# 默认使用GPU \n",
    "def load_model(task, save_pth, lm, use_gpu=True):\n",
    "    \n",
    "    # load models\n",
    "    checkpoint = os.path.join(save_pth, task, 'model.pt')\n",
    "    if not os.path.exists(checkpoint):\n",
    "        raise ModelNotFoundError(checkpoint)\n",
    "\n",
    "    if use_gpu:\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    else:\n",
    "        device = 'cpu'\n",
    "\n",
    "    model = DittoModel(device=device, lm=lm)\n",
    "\n",
    "    saved_state = torch.load(checkpoint, map_location=lambda storage, loc: storage)\n",
    "    model.load_state_dict(saved_state['model'])\n",
    "    model = model.to(device)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_dataframes(ent1, ent2):\n",
    "    rows_1 = []\n",
    "    rows_2 = []\n",
    "    rows = []\n",
    "\n",
    "    content = ''\n",
    "    for _, row in ent1.iterrows():\n",
    "        # 读取属性\n",
    "        \n",
    "        for attr,val in zip(row.index, row.values):\n",
    "            content += 'COL %s VAL %s ' % (str(attr), str(val))\n",
    "        \n",
    "        content += '\\t'\n",
    "        rows_1.append(content)\n",
    "        content = ''\n",
    "\n",
    "    for _, row in ent2.iterrows():\n",
    "        # 读取属性\n",
    "        \n",
    "        for attr,val in zip(row.index, row.values):\n",
    "            content += 'COL %s VAL %s ' % (str(attr), str(val))\n",
    "        \n",
    "        content += '\\t'\n",
    "        rows_2.append(content)\n",
    "        content = ''\n",
    "        \n",
    "    for row_l in rows_1:\n",
    "        left = row_l.split('\\t')[0]\n",
    "        for row_r in rows_2:\n",
    "            right = row_r.split('\\t')[0]\n",
    "            rows.append(left + '\\t ' + right)\n",
    "\n",
    "    return rows\n",
    "\n",
    "def combine_string(rows_l, rows_r):\n",
    "\n",
    "    rows = []\n",
    "        \n",
    "    for left in rows_l:\n",
    "        # left = row_l.split('\\t')[0]\n",
    "        for right in rows_r:\n",
    "            # right = row_r.split('\\t')[0]\n",
    "            rows.append(left + '\\t ' + right)\n",
    "\n",
    "    return rows\n",
    "\n",
    "\n",
    "\n",
    "# 处理多样化的输入数据：\n",
    "# \n",
    "def get_input(content,type='string'):\n",
    "    # 判断使用那种输入方式？\n",
    "    if type =='file':\n",
    "        # txt or json\n",
    "        input_path = content\n",
    "        if len(content)==2 and '.csv' in content[0] and '.csv' in content[0]:\n",
    "            # 传入了2个csv文件\n",
    "            path_1 = content[0]\n",
    "            df1 = pd.read_csv(path_1)\n",
    "            path_2 = content[1]\n",
    "            df2 = pd.read_csv(path_2)\n",
    "            rows = combine_dataframes(df1,df2)\n",
    "            return rows\n",
    "\n",
    "        if '.txt' in input_path:\n",
    "            with jsonlines.open(input_path + '.jsonl', mode='w') as writer:\n",
    "                for line in open(input_path):\n",
    "                    writer.write(line.split('\\t')[:2])\n",
    "            input_path += '.jsonl'\n",
    "\n",
    "        # batch processing\n",
    "        # start_time = time.time()\n",
    "        with jsonlines.open(input_path) as reader: # writer向文件写入\n",
    "            rows = []\n",
    "            for _, row in enumerate(reader):\n",
    "                # rows.append(to_str(row[0], row[1], summarizer, max_len))\n",
    "                rows.append(row)\n",
    "        # 删除文件\n",
    "        if '.txt' in input_path:\n",
    "            # cmd  = 'rm {}'.format(input_path[:-6]) \n",
    "            # print('remove', input_path[:-6])\n",
    "            cmd  = 'rm {}'.format(input_path)\n",
    "            print('remove', input_path)\n",
    "            os.system(cmd)\n",
    "            \n",
    "        return rows\n",
    "        \n",
    "    # csv\n",
    "    elif type =='dataframe':\n",
    "        # 得到了两个df\n",
    "        \n",
    "        # df1 = content[0]\n",
    "        # df2 = content[1]\n",
    "        # rows = combine_dataframes(df1,df2)\n",
    "        \n",
    "        # return rows\n",
    "        pass\n",
    "    elif type =='string':\n",
    "        # 得到了两个 string\n",
    "        \n",
    "        rows_l = content[0].split('\\n')\n",
    "        rows_r = content[1].split('\\n')\n",
    "        \n",
    "        rows = combine_string(rows_l, rows_r)\n",
    "        return rows\n",
    "    else:\n",
    "        print('TYPE ERROR')\n",
    "        return None\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    # # jsonl 文件\n",
    "    \n",
    "    # # csv表格\n",
    "    \n",
    "    # # string 直接输入\n",
    "    \n",
    "    # pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--task\", type=str, default='Structured/Beer')\n",
    "    parser.add_argument(\"--input_path\", type=str, default='input/candidates.jsonl')\n",
    "    parser.add_argument(\"--output_path\", type=str, default='output/match_candidates.jsonl')\n",
    "    parser.add_argument(\"--lm\", type=str, default='distilbert')\n",
    "    parser.add_argument(\"--use_gpu\", dest=\"use_gpu\", action=\"store_true\")\n",
    "\n",
    "    parser.add_argument(\"--checkpoint_path\", type=str, default='checkpoints/')\n",
    "    parser.add_argument(\"--dk\", type=str, default=None)\n",
    "    parser.add_argument(\"--summarize\", dest=\"summarize\", action=\"store_true\")\n",
    "    parser.add_argument(\"--max_len\", type=int, default=256)\n",
    "    hp = parser.parse_args()\n",
    "\n",
    "    # load the models\n",
    "    set_seed(123)\n",
    "\n",
    "    model = load_model(hp.task, hp.checkpoint_path,\n",
    "                       hp.lm, hp.use_gpu)\n",
    "\n",
    "\n",
    "    # 来自交互 gradio\n",
    "    # content, type = XXXX\n",
    "    \n",
    "    # input_rows = pass\n",
    "    input_rows = get_input(content, type)\n",
    "    \n",
    "    predict(input_rows, hp.output_path, model,\n",
    "            summarizer=summarizer,\n",
    "            max_len=hp.max_len,\n",
    "            lm=hp.lm,\n",
    "            dk_injector=dk_injector,\n",
    "            threshold=threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# path = 'candidate.txt'\n",
    "# content = ''\n",
    "# with open(path, 'r') as file:\n",
    "#     for line in file:\n",
    "#         content += line\n",
    "#         # print(line)\n",
    "        \n",
    "# content1 = content\n",
    "# content2 = content\n",
    "\n",
    "# len(get_input([content1, content2], 'string')) # 8*8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['COL title VAL secure transaction processing in firm real-time database systems binto george , jayant r. haritsa sigmod conference COL authors VAL  COL venue VAL  COL year VAL 1997.0 ',\n",
       " 'COL title VAL secure buffering in firm real-time database systems 2000 COL authors VAL binto george , jayant r. haritsa COL venue VAL the vldb journal -- the international journal on very large data bases COL year VAL  ']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content.split('\\n')[0].split('\\t')[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# content = 'candidate.jsonl'\n",
    "# rows = get_input(content, 'file')\n",
    "\n",
    "# content = 'candidate.txt'\n",
    "# rows_1 = get_input(content, 'file')\n",
    "# rows_1[0]\n",
    "\n",
    "# content = ['can_a.csv', 'can_b.csv']\n",
    "# rows_1 = get_input(content, 'file')\n",
    "# rows_1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # 假设 ent1 和 ent2 是 DataFrame 数据\n",
    "# # 创建示例 DataFrame\n",
    "# ent1 = pd.DataFrame({'attr1': [1, 2, 3], 'attr2': ['A', 'B', 'C']})\n",
    "# ent2 = pd.DataFrame({'attr1': [4, 5, 6], 'attr2': ['D', 'E', 'F']})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_dataframe(ent1, ent2):\n",
    "#     rows_1 = []\n",
    "#     rows_2 = []\n",
    "#     rows = []\n",
    "\n",
    "#     content = ''\n",
    "#     for _, row in ent1.iterrows():\n",
    "#         # 读取属性\n",
    "        \n",
    "#         for attr,val in zip(row.index, row.values):\n",
    "#             print(attr,val)\n",
    "#             content += 'COL %s VAL %s ' % (str(attr), str(val))\n",
    "        \n",
    "#         content += '\\t'\n",
    "#         rows_1.append(content)\n",
    "#         content = ''\n",
    "\n",
    "#     for _, row in ent2.iterrows():\n",
    "#         # 读取属性\n",
    "        \n",
    "#         for attr,val in zip(row.index, row.values):\n",
    "#             content += 'COL %s VAL %s ' % (str(attr), str(val))\n",
    "        \n",
    "#         content += '\\t'\n",
    "#         rows_2.append(content)\n",
    "#         content = ''\n",
    "        \n",
    "#     for row_l in rows_1:\n",
    "#         left = row_l.split('\\t')[0]\n",
    "#         for row_r in rows_2:\n",
    "#             right = row_r.split('\\t')[0]\n",
    "#             rows.append(left + '\\t ' + right)\n",
    "\n",
    "#     return rows\n",
    "\n",
    "\n",
    "# get_dataframe(ent1, ent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# content = ''\n",
    "# for ent in [ent1, ent2]:\n",
    "#     if isinstance(ent, pd.DataFrame):\n",
    "#         for index, row in ent.iterrows():\n",
    "#             for attr in ent.columns:\n",
    "#                 content += 'COL %s VAL %s ' % (attr, row[attr])\n",
    "#             content += '\\t'\n",
    "#     else:\n",
    "#         if isinstance(ent, str):\n",
    "#             content += ent\n",
    "#         else:\n",
    "#             for attr in ent.keys():\n",
    "#                 content += 'COL %s VAL %s ' % (attr, ent[attr])\n",
    "#         content += '\\t'\n",
    "\n",
    "# content += '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
